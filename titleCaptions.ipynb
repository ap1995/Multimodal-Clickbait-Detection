{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jsonlines\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "# VALIDATION_SPLIT = 0.1111806\n",
    "train_val_data =[]\n",
    "truth_data = []\n",
    "test_data = []\n",
    "count= 0\n",
    "full_count=0\n",
    "with jsonlines.open('instances.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        count += 1\n",
    "        full_count+=1\n",
    "        if (count > 17600):\n",
    "            test_data.append(obj)\n",
    "        if(count<=17600):\n",
    "            train_val_data.append(obj)\n",
    "\n",
    "count = 0\n",
    "truth_data = []\n",
    "with jsonlines.open('truth.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        truth_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"@\", \"\", string)\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(vals_df):\n",
    "    labels = []\n",
    "    for i in vals_df.values:\n",
    "        if(i[3]==\"clickbait\"):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_df(vals):\n",
    "    titles_df = []\n",
    "    for i in range(len(vals)): ## For titles\n",
    "        text = []\n",
    "        k = vals[i][2]\n",
    "        text+=(k)\n",
    "        words = \"\"\n",
    "        for string in text:\n",
    "            string = clean_str(string)\n",
    "            words +=\" \".join(string.split())\n",
    "        titles_df+=[words]\n",
    "    return titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption_df(vals):\n",
    "    titles_df = []\n",
    "    for i in range(len(vals)): ## For titles\n",
    "        text = []\n",
    "        k = vals[i][4]\n",
    "        text+=(k)\n",
    "        words = \"\"\n",
    "        for string in text:\n",
    "            string = clean_str(string)\n",
    "            words +=\" \".join(string.split())\n",
    "        titles_df+=[words]\n",
    "    return titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_df(vals):\n",
    "    content_df = []\n",
    "    for i in range(len(vals)): ## For content\n",
    "        text = []\n",
    "        for j in range(2, 6):\n",
    "            if(j==5):\n",
    "                continue\n",
    "            else:\n",
    "                k = vals[i][j]\n",
    "                if(j==6):\n",
    "                    text.append(k)\n",
    "                else:\n",
    "                    text += (k)\n",
    "        words = \"\"\n",
    "        for string in text:\n",
    "            string = clean_str(string)\n",
    "            words += \" \".join(string.split())\n",
    "        content_df += [words]\n",
    "    return content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences(df):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df)\n",
    "    sequences = tokenizer.texts_to_sequences(df)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return data, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9295\n"
     ]
    }
   ],
   "source": [
    "final_vals = []\n",
    "data_df = pd.DataFrame.from_dict(train_val_data)\n",
    "truth_data_df = pd.DataFrame.from_dict(truth_data)\n",
    "train_captions_df = pd.read_csv('./total_captions.csv')\n",
    "train_captions_df['id'] = train_captions_df['id'].apply(str)\n",
    "# features = [\"id\", \"postMedia\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "#                 \"targetDescription\", \"truthClass\"]\n",
    "train_1 = pd.merge(data_df, truth_data_df, on=\"id\")\n",
    "train = pd.merge(train_1,train_captions_df,on=\"id\")\n",
    "print(len(train))\n",
    "features = [\"id\",\"postMedia\",\"targetTitle\",  \"truthClass\",\"caption\"]\n",
    "vals = train[features]\n",
    "vals = vals.values.tolist()\n",
    "for i in range(len(vals)):\n",
    "    if vals[i][1] != []:\n",
    "        final_vals.append([vals[i][0],vals[i][1],vals[i][2], vals[i][3], vals[i][4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vals length 9295\n"
     ]
    }
   ],
   "source": [
    "vals_df = pd.DataFrame(final_vals, columns=[\"id\", \"postMedia\",\"targetTitle\",  \"truthClass\",\"caption\"])\n",
    "print(\"Final vals length\", len(final_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalTestVals length 1011\n"
     ]
    }
   ],
   "source": [
    "finalTestvals = []\n",
    "test_data_df = pd.DataFrame.from_dict(test_data)\n",
    "test_1 = pd.merge(test_data_df, truth_data_df, on=\"id\")\n",
    "test = pd.merge(test_1,train_captions_df,on=\"id\")\n",
    "test_vals = test[features].values.tolist()\n",
    "for i in range(len(test_vals)):\n",
    "    if test_vals[i][1] != []:\n",
    "        finalTestvals.append([test_vals[i][0],test_vals[i][1],test_vals[i][2], test_vals[i][3], test_vals[i][4]])\n",
    "test_vals_df = pd.DataFrame(finalTestvals, columns=[\"id\", \"postMedia\",\"targetTitle\",  \"truthClass\",\"caption\"])\n",
    "print(\"finalTestVals length\", len(finalTestvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(vals_df)\n",
    "tlabels = get_labels(test_vals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9283 unique tokens.\n",
      "Found 3456 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "title_train = get_title_df(vals_df.values.tolist())\n",
    "content_train = get_caption_df(vals_df.values.tolist())\n",
    "\n",
    "title_train_df, t_word_index = get_padded_sequences(title_train)\n",
    "content_train_df, c_word_index = get_padded_sequences(content_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1036 unique tokens.\n",
      "Found 631 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "title_test = get_title_df(test_vals_df.values.tolist())\n",
    "content_test = get_caption_df(test_vals_df.values.tolist())\n",
    "\n",
    "title_test_df, title_test_index = get_padded_sequences(title_test)\n",
    "content_test_df, content_test_index = get_padded_sequences(content_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (9295, 1000)\n",
      "Shape of label tensor: (9295, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', title_train_df.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "VALIDATION_SPLIT = 0.1111806\n",
    "indices = np.arange(title_train_df.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = title_train_df[indices]\n",
    "content_data = content_train_df[indices]\n",
    "# image_data = image_features[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_title_train = data[:-nb_validation_samples]\n",
    "x_content_train = content_data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "\n",
    "x_title_val = data[-nb_validation_samples:]\n",
    "x_content_val = content_data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_title_test = title_test_df\n",
    "x_content_test = content_test_df\n",
    "y_test = tlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation sets\n",
      "[6512. 1750.]\n",
      "[806. 227.]\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Training and validation sets')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "\n",
    "embeddings_index = {}\n",
    "f=open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_embedding_matrix = np.random.random((len(t_word_index) + 1, EMBEDDING_DIM)) ##Titles\n",
    "# for word, i in t_word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         t_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# c_embedding_matrix = np.random.random((len(c_word_index) + 1, EMBEDDING_DIM)) ##Content\n",
    "# for word, i in c_word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         c_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embedding_matrix = np.random.random((len(title_test_index) + 1, EMBEDDING_DIM)) ##Titles\n",
    "for word, i in title_test_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        t_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "c_embedding_matrix = np.random.random((len(content_test_index) + 1, EMBEDDING_DIM)) ##Content\n",
    "for word, i in content_test_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        c_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = Embedding(len(t_word_index) + 1, EMBEDDING_DIM, weights=[t_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=True)\n",
    "\n",
    "# content_embedding_layer = Embedding(len(c_word_index) + 1, EMBEDDING_DIM, weights=[c_embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(title_test_index) + 1, EMBEDDING_DIM, weights=[t_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "content_embedding_layer = Embedding(len(content_test_index) + 1, EMBEDDING_DIM, weights=[c_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "content_data_input = Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='float32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "\n",
    "content_embedded_sequences = content_embedding_layer(content_data_input)\n",
    "l_lstm_content = Bidirectional(LSTM(100))(content_embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Shape.0\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------\")\n",
    "print(l_lstm.shape)\n",
    "\n",
    "preds_title = Dense(2, activation='softmax')(l_lstm)\n",
    "\n",
    "preds_content = Dense(2,activation='softmax')(l_lstm_content)\n",
    "\n",
    "preds_add = concatenate([preds_title, preds_content], axis =-1)\n",
    "\n",
    "preds = Dense(2)(preds_add)\n",
    "\n",
    "model = Model([sequence_input, content_data_input], preds)\n",
    "# model1.add_update(ac.AttentionWithContext()) ###############\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"weights-text-{epoch:02d}-{val_acc:.2f}.hdf5\")\n",
    "callbacks_list = [checkpoint]\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Bidirectional LSTM with titles and content\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    103700      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1000, 100)    63200       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200)          160800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200)          160800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            402         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            402         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            10          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 489,314\n",
      "Trainable params: 489,314\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "print(\"model fitting - Bidirectional LSTM with titles and content\")\n",
    "model.summary()\n",
    "print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.fit([x_title_train, x_content_train], y_train, validation_data=([x_title_val, x_content_val], y_val), epochs=10, batch_size=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights-text-10-0.79.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011/1011 [==============================] - 20s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([x_title_test, x_content_test], batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on Test data  0.2354104846686449\n"
     ]
    }
   ],
   "source": [
    "preds_new = []\n",
    "for i in range(len(preds)):\n",
    "    preds_new.append(preds[i][0] + preds[i][1])\n",
    "print(\"Accuracy score on Test data \", accuracy_score(y_test, np.asarray(preds_new).round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
