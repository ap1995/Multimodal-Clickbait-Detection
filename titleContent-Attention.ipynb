{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jsonlines\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import AttentionwithContext as ac\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1111806\n",
    "train_val_data =[]\n",
    "truth_data = []\n",
    "test_data = []\n",
    "count= 0\n",
    "full_count=0\n",
    "with jsonlines.open('instances_train.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        count += 1\n",
    "        full_count+=1\n",
    "        if (count > 17600):\n",
    "            test_data.append(obj)\n",
    "        if(count<=17600):\n",
    "            train_val_data.append(obj)\n",
    "\n",
    "count = 0\n",
    "truth_data = []\n",
    "with jsonlines.open('truth.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        truth_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"@\", \"\", string)\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(vals_df):\n",
    "    labels = []\n",
    "    for i in vals_df.values:\n",
    "        if(i[7]==\"clickbait\"):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_df(vals):\n",
    "    titles_df = []\n",
    "    for i in range(len(vals)): ## For titles\n",
    "        text = []\n",
    "        k = vals[i][4]\n",
    "        text.append(k)\n",
    "        words = \"\"\n",
    "        for string in text:\n",
    "            string = clean_str(string)\n",
    "            words +=\" \".join(string.split())\n",
    "        titles_df+=[words]\n",
    "    return titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_df(vals):\n",
    "    content_df = []\n",
    "    for i in range(len(vals)): ## For content\n",
    "        text = []\n",
    "        for j in range(2, 6):\n",
    "            if(j==4):\n",
    "                continue\n",
    "            else:\n",
    "                k = vals[i][j]\n",
    "                if(j==6):\n",
    "                    text.append(k)\n",
    "                else:\n",
    "                    text += (k)\n",
    "        words = \"\"\n",
    "        for string in text:\n",
    "            string = clean_str(string)\n",
    "            words += \" \".join(string.split())\n",
    "        content_df += [words]\n",
    "    return content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences(df):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df)\n",
    "    sequences = tokenizer.texts_to_sequences(df)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return data, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vals = []\n",
    "data_df = pd.DataFrame.from_dict(train_val_data)\n",
    "truth_data_df = pd.DataFrame.from_dict(truth_data)\n",
    "train = pd.merge(data_df, truth_data_df, on=\"id\")\n",
    "features = [\"id\", \"postMedia\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\", \"truthClass\"]\n",
    "vals = train[features]\n",
    "vals = vals.values.tolist()\n",
    "for i in range(len(vals)):\n",
    "    if vals[i][1] != []:\n",
    "        final_vals.append([vals[i][0], vals[i][2], vals[i][3], vals[i][4], vals[i][5], vals[i][6], vals[i][7], vals[i][8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vals length 9295\n"
     ]
    }
   ],
   "source": [
    "vals_df = pd.DataFrame(final_vals, columns=[\"id\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\", \"truthClass\"])\n",
    "print(\"Final vals length\", len(final_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalTestVals length 1011\n"
     ]
    }
   ],
   "source": [
    "finalTestvals = []\n",
    "test_data_df = pd.DataFrame.from_dict(test_data)\n",
    "test = pd.merge(test_data_df, truth_data_df, on=\"id\")\n",
    "test_vals = test[features].values.tolist()\n",
    "for i in range(len(test_vals)):\n",
    "    if test_vals[i][1] != []:\n",
    "        finalTestvals.append([test_vals[i][0], test_vals[i][2], test_vals[i][3], test_vals[i][4], test_vals[i][5], test_vals[i][6], test_vals[i][7], test_vals[i][8]])\n",
    "\n",
    "test_vals_df = pd.DataFrame(finalTestvals, columns=[\"id\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\", \"truthClass\"])\n",
    "print(\"finalTestVals length\", len(finalTestvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(vals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlabels = get_labels(test_vals_df) #For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15837 unique tokens.\n",
      "Found 245526 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "title_train = get_title_df(vals_df.values.tolist())\n",
    "content_train = get_content_df(vals_df.values.tolist())\n",
    "\n",
    "title_train_df, t_word_index = get_padded_sequences(title_train)\n",
    "content_train_df, c_word_index = get_padded_sequences(content_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4424 unique tokens.\n",
      "Found 54783 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "title_test = get_title_df(test_vals_df.values.tolist()) ##For testing\n",
    "content_test = get_content_df(test_vals_df.values.tolist())\n",
    "\n",
    "title_test_df, title_test_index = get_padded_sequences(title_test)\n",
    "content_test_df, content_test_index = get_padded_sequences(content_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (9295, 200)\n",
      "Shape of label tensor: (9295, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', title_train_df.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "indices = np.arange(title_train_df.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = title_train_df[indices]\n",
    "content_data = content_train_df[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_title_train = data[:-nb_validation_samples]\n",
    "x_content_train = content_data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "\n",
    "x_title_val = data[-nb_validation_samples:]\n",
    "x_content_val = content_data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_title_test = title_test_df ##For testing\n",
    "x_content_test = content_test_df\n",
    "y_test = tlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation sets\n",
      "[ 6513.  1749.]\n",
      "[ 805.  228.]\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Training and validation sets')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "\n",
    "embeddings_index = {}\n",
    "f=open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embedding_matrix = np.random.random((len(t_word_index) + 1, EMBEDDING_DIM)) ##Titles\n",
    "for word, i in t_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        t_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "c_embedding_matrix = np.random.random((len(c_word_index) + 1, EMBEDDING_DIM)) ##Content\n",
    "for word, i in c_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        c_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.concatenate((t_embedding_matrix, c_embedding_matrix), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(t_word_index)+ len(c_word_index) + 2, EMBEDDING_DIM, weights=[weights_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/layers/recurrent.py:2024: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n",
      "/usr/local/lib/python3.5/dist-packages/keras/layers/recurrent.py:2024: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32') #Run again while testing\n",
    "content_data_input = Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='float32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "\n",
    "content_embedded_sequences = embedding_layer(content_data_input)\n",
    "l_lstm_content = Bidirectional(LSTM(100))(content_embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "attention_probs = Dense(MAX_SEQUENCE_LENGTH, activation='softmax', name='attention_probs')(inputs)\n",
    "attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "\n",
    "# def mean_pred(y_true, y_pred):\n",
    "#     return K.mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwini/.local/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/ashwini/.local/lib/python3.5/site-packages/ipykernel_launcher.py:19: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "preds_title = Dense(1, activation='softmax')(l_lstm) #Run again while testing\n",
    "attention_probs_title = Dense(MAX_SEQUENCE_LENGTH, activation='softmax', name='attention_probs')(preds_title)\n",
    "attention_mul = merge([sequence_input, attention_probs_title], output_shape=32, name='attention_mul', mode='mul')\n",
    "preds_title = Dense(1, activation='softmax')(attention_mul)\n",
    "\n",
    "preds_content = Dense(1,activation='softmax')(l_lstm_content)\n",
    "\n",
    "# preds_add = concatenate([preds_title, preds_content], axis =-1)\n",
    "\n",
    "# preds = Dense(2)(preds_add)\n",
    "\n",
    "# model = Model([sequence_input, content_data_input], preds)\n",
    "# model2.add(ac.AttentionWithContext()) ###############\n",
    "\n",
    "# attention_vector = get_activations(m, testing_inputs_1, print_shape_only=True)[1].flatten()\n",
    "\n",
    "model1 = Model(sequence_input, preds_title)\n",
    "model2 = Model(content_data_input, preds_content)\n",
    "merged = Merge([model1, model2], mode='concat', name=\"merged\")\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "# model = Sequential()\n",
    "# model.add(merged)\n",
    "# model3 = Model(inputs, attention_mul)\n",
    "# final_model = Sequential()\n",
    "# final_model.add(Merge([model, model3], mode = 'concat'))\n",
    "# final_model.add(attention_mul)\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "rmsprop = RMSprop(lr=0.005, rho=0.9, epsilon=None, decay=0.0004)\n",
    "checkpoint = ModelCheckpoint(\"weights-content-{epoch:02d}-{val_acc:.2f}.hdf5\")\n",
    "callbacks_list = [checkpoint]\n",
    "final_model.compile(loss='mean_squared_error', optimizer=rmsprop, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Bidirectional LSTM with titles and content\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merged (Merge)               (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 26,459,103\n",
      "Trainable params: 26,459,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "print(\"model fitting - Bidirectional LSTM with titles and content\")\n",
    "final_model.summary()\n",
    "print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8262 samples, validate on 1033 samples\n",
      "Epoch 1/3\n",
      "3750/8262 [============>.................] - ETA: 6:15 - loss: 0.5000 - acc: 0.7845"
     ]
    }
   ],
   "source": [
    "final_model.fit([x_title_train, x_content_train], y_train, validation_data=([x_title_val, x_content_val], y_val), epochs=3, batch_size=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.load_weights('weights-content2-01-0.78.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_model.predict_classes([x_title_test, x_content_test], batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score on Test data \", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
