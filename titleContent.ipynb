{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jsonlines\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "# VALIDATION_SPLIT = 0.1111806\n",
    "train_val_data =[]\n",
    "truth_data = []\n",
    "test_data = []\n",
    "count= 0\n",
    "full_count=0\n",
    "with jsonlines.open('instances.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        count += 1\n",
    "        full_count+=1\n",
    "        if (count > 17600):\n",
    "            test_data.append(obj)\n",
    "        if(count<=17600):\n",
    "            train_val_data.append(obj)\n",
    "\n",
    "count = 0\n",
    "truth_data = []\n",
    "with jsonlines.open('truth.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        truth_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"@\", \"\", string)\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(vals_df):\n",
    "    labels = []\n",
    "    for i in vals_df.values:\n",
    "        if(i[7]==\"clickbait\"):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_df(vals):\n",
    "    titles_df = []\n",
    "    for i in range(len(vals)): ## For titles\n",
    "        text = []\n",
    "        k = vals[i][5]\n",
    "        text+=(k)\n",
    "        words = \"\"\n",
    "        for string in text:\n",
    "            string = clean_str(string)\n",
    "            words +=\" \".join(string.split())\n",
    "        titles_df+=[words]\n",
    "    return titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_df(vals):\n",
    "    content_df = []\n",
    "    for i in range(len(vals)): ## For content\n",
    "        text = []\n",
    "        for j in range(2, 6):\n",
    "            if(j==5):\n",
    "                continue\n",
    "            else:\n",
    "                k = vals[i][j]\n",
    "                if(j==6):\n",
    "                    text.append(k)\n",
    "                else:\n",
    "                    text += (k)\n",
    "        words = \"\"\n",
    "        for string in text:\n",
    "            string = clean_str(string)\n",
    "            words += \" \".join(string.split())\n",
    "        content_df += [words]\n",
    "    return content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences(df):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df)\n",
    "    sequences = tokenizer.texts_to_sequences(df)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return data, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vals = []\n",
    "data_df = pd.DataFrame.from_dict(train_val_data)\n",
    "truth_data_df = pd.DataFrame.from_dict(truth_data)\n",
    "train = pd.merge(data_df, truth_data_df, on=\"id\")\n",
    "features = [\"id\", \"postMedia\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\", \"truthClass\"]\n",
    "vals = train[features]\n",
    "vals = vals.values.tolist()\n",
    "for i in range(len(vals)):\n",
    "    if vals[i][1] != []:\n",
    "        final_vals.append([vals[i][0], vals[i][2], vals[i][3], vals[i][4], vals[i][5], vals[i][6], vals[i][7], vals[i][8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vals length 9295\n"
     ]
    }
   ],
   "source": [
    "vals_df = pd.DataFrame(final_vals, columns=[\"id\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\", \"truthClass\"])\n",
    "print(\"Final vals length\", len(final_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalTestVals length 1011\n"
     ]
    }
   ],
   "source": [
    "finalTestvals = []\n",
    "test_data_df = pd.DataFrame.from_dict(test_data)\n",
    "test = pd.merge(test_data_df, truth_data_df, on=\"id\")\n",
    "test_vals = test[features].values.tolist()\n",
    "for i in range(len(test_vals)):\n",
    "    if test_vals[i][1] != []:\n",
    "        finalTestvals.append([test_vals[i][0], test_vals[i][2], test_vals[i][3], test_vals[i][4], test_vals[i][5], test_vals[i][6], test_vals[i][7], test_vals[i][8]])\n",
    "\n",
    "test_vals_df = pd.DataFrame(finalTestvals, columns=[\"id\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\", \"truthClass\"])\n",
    "print(\"finalTestVals length\", len(finalTestvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(vals_df)\n",
    "tlabels = get_labels(test_vals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4242 unique tokens.\n",
      "Found 250177 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "title_train = get_title_df(vals_df.values.tolist())\n",
    "content_train = get_content_df(vals_df.values.tolist())\n",
    "\n",
    "title_train_df, t_word_index = get_padded_sequences(title_train)\n",
    "content_train_df, c_word_index = get_padded_sequences(content_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 515 unique tokens.\n",
      "Found 55270 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "title_test = get_title_df(test_vals_df.values.tolist())\n",
    "content_test = get_content_df(test_vals_df.values.tolist())\n",
    "\n",
    "title_test_df, title_test_index = get_padded_sequences(title_test)\n",
    "content_test_df, content_test_index = get_padded_sequences(content_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (9295, 1000)\n",
      "Shape of label tensor: (9295, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', title_train_df.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "VALIDATION_SPLIT = 0.1111806\n",
    "indices = np.arange(title_train_df.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = title_train_df[indices]\n",
    "content_data = content_train_df[indices]\n",
    "# image_data = image_features[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_title_train = data[:-nb_validation_samples]\n",
    "x_content_train = content_data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "\n",
    "x_title_val = data[-nb_validation_samples:]\n",
    "x_content_val = content_data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_title_test = title_test_df\n",
    "x_content_test = content_test_df\n",
    "y_test = tlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation sets\n",
      "[ 6497.  1765.]\n",
      "[ 821.  212.]\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Training and validation sets')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "\n",
    "embeddings_index = {}\n",
    "f=open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embedding_matrix = np.random.random((len(t_word_index) + 1, EMBEDDING_DIM)) ##Titles\n",
    "for word, i in t_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        t_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "c_embedding_matrix = np.random.random((len(c_word_index) + 1, EMBEDDING_DIM)) ##Content\n",
    "for word, i in c_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        c_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embedding_matrix = np.random.random((len(title_test_index) + 1, EMBEDDING_DIM)) ##Titles\n",
    "for word, i in title_test_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        t_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "c_embedding_matrix = np.random.random((len(content_test_index) + 1, EMBEDDING_DIM)) ##Content\n",
    "for word, i in content_test_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        c_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(t_word_index) + 1, EMBEDDING_DIM, weights=[t_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "content_embedding_layer = Embedding(len(c_word_index) + 1, EMBEDDING_DIM, weights=[c_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(title_test_index) + 1, EMBEDDING_DIM, weights=[t_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "content_embedding_layer = Embedding(len(content_test_index) + 1, EMBEDDING_DIM, weights=[c_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/layers/recurrent.py:2024: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n",
      "/usr/local/lib/python3.5/dist-packages/keras/layers/recurrent.py:2024: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "content_data_input = Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='float32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "\n",
    "content_embedded_sequences = content_embedding_layer(content_data_input)\n",
    "l_lstm_content = Bidirectional(LSTM(100))(content_embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------------------\")\n",
    "print(l_lstm.shape)\n",
    "\n",
    "preds_title = Dense(2, activation='softmax')(l_lstm)\n",
    "\n",
    "preds_content = Dense(2,activation='softmax')(l_lstm_content)\n",
    "\n",
    "preds_add = concatenate([preds_title, preds_content], axis =-1)\n",
    "\n",
    "preds = Dense(2)(preds_add)\n",
    "\n",
    "model = Model([sequence_input, content_data_input], preds)\n",
    "# model1.add_update(ac.AttentionWithContext()) ###############\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"weights-text-{epoch:02d}-{val_acc:.2f}.hdf5\")\n",
    "callbacks_list = [checkpoint]\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model fitting - Bidirectional LSTM with titles and content\")\n",
    "model.summary()\n",
    "print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.fit([x_title_train, x_content_train], y_train, validation_data=([x_title_val, x_content_val], y_val), epochs=10, batch_size=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
