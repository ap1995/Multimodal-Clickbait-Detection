{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jsonlines\n",
    "import os\n",
    "import random\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.merge import concatenate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Embedding, Merge, Dropout, Flatten, LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "import AttentionwithContext as ac\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow as tf\n",
    "from config import Config\n",
    "from cnn_model import cnn_model\n",
    "import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200 #30\n",
    "MAX_CONTENT_LENGTH = 200\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgModel(vals_df):\n",
    "    img_features = []\n",
    "    config = Config()\n",
    "    images = tf.placeholder(dtype=tf.float32, shape=[config.batch_size] + config.image_shape)\n",
    "    sess = tf.Session()\n",
    "\n",
    "    model = cnn_model(config)\n",
    "    features = model.build_vgg16(images)\n",
    "    model.load_cnn(sess,config.vgg16_file)\n",
    "\n",
    "    for entry in vals_df.values:\n",
    "        img_path = entry[1][0]\n",
    "        try:\n",
    "            img = image.load_img(img_path, target_size=(224, 224,3))\n",
    "        except OSError:\n",
    "            continue\n",
    "        img_data = image.img_to_array(img)\n",
    "        img_data = np.expand_dims(img_data, axis=0)\n",
    "        img_data = preprocess_input(img_data)\n",
    "        imgList = img_data\n",
    "        vgg16_feature = sess.run(features,feed_dict={images:img_data})\n",
    "        img_features.append(vgg16_feature[0])\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    return img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"@\", \"\", string)\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_df(vals):\n",
    "    titles_df = []\n",
    "    for i in range(len(vals)): ## For titles\n",
    "        text = []\n",
    "        k = vals[i][5]\n",
    "        text.append(k)\n",
    "        words = \"\"\n",
    "        for string in text:\n",
    "            string = clean_str(string)\n",
    "            words +=\" \".join(string.split())\n",
    "        titles_df+=[words]\n",
    "    return titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_df(vals):\n",
    "    content_df = []\n",
    "    for i in range(len(vals)): ## For content\n",
    "        text = []\n",
    "        for j in range(2, 7):\n",
    "            if(j==5):\n",
    "                continue\n",
    "            else:\n",
    "                k = vals[i][j]\n",
    "                if(j==7):\n",
    "                    text.append(k)\n",
    "                else:\n",
    "                    text += (k)\n",
    "        words = \"\"\n",
    "        for string in text:\n",
    "            string = clean_str(string)\n",
    "            words += \" \".join(string.split())\n",
    "        content_df += [words]\n",
    "    return content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences(df):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df)\n",
    "    sequences = tokenizer.texts_to_sequences(df)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_CONTENT_LENGTH)\n",
    "    return data, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences_content(df):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(df)\n",
    "    sequences = tokenizer.texts_to_sequences(df)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_CONTENT_LENGTH)\n",
    "    return data, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80013\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "full_count = 0\n",
    "train_val_data = []\n",
    "test_data = []\n",
    "complete_data = []\n",
    "\n",
    "with jsonlines.open('instances.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        count += 1\n",
    "        complete_data.append(obj)\n",
    "random.shuffle(complete_data)\n",
    "complete_length = len(complete_data)\n",
    "print(complete_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vals = []\n",
    "data_df = pd.DataFrame.from_dict(complete_data)\n",
    "# train = pd.merge(data_df, truth_data_df, on=\"id\")\n",
    "features = [\"id\", \"postMedia\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\"]\n",
    "vals = data_df[features]\n",
    "vals = vals.values.tolist()\n",
    "# length = 5000\n",
    "count =0\n",
    "for i in range(len(vals)):\n",
    "    if vals[i][1] != []:\n",
    "        final_vals.append([vals[i][0], [vals[i][1][0]], vals[i][2], vals[i][3], vals[i][4], vals[i][5], vals[i][6], vals[i][7]])\n",
    "#         count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vals length 36042\n"
     ]
    }
   ],
   "source": [
    "vals_df = pd.DataFrame(final_vals, columns=[\"id\", \"postMedia\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\"])\n",
    "print(\"Final vals length\", len(final_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the CNN from ./vgg16_no_fc.npy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 4/13 [00:00<00:00, 38.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv3_3\n",
      "conv1_2\n",
      "conv3_1\n",
      "conv5_2\n",
      "conv1_1\n",
      "conv4_3\n",
      "conv4_1\n",
      "conv5_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 36.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2_1\n",
      "conv2_2\n",
      "conv3_2\n",
      "conv5_1\n",
      "conv4_2\n",
      "26 tensors loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_features = imgModel(vals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36033\n"
     ]
    }
   ],
   "source": [
    "print(len(image_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27823 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "title_unlabeled = get_title_df(vals_df.values.tolist())\n",
    "title_unlabeled_df, t_word_index = get_padded_sequences(title_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 517052 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "content_unlabeled = get_content_df(vals_df.values.tolist())\n",
    "content_unlabeled_df, c_word_index = get_padded_sequences(content_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(content_unlabeled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0  ### LABELED DATA\n",
    "full_count = 0\n",
    "train_val_data = []\n",
    "test_data = []\n",
    "all_data = []\n",
    "\n",
    "with jsonlines.open('instances_train.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "#         count += 1\n",
    "#         full_count+=1\n",
    "#         if (count > 17600):\n",
    "#             test_data.append(obj)\n",
    "#         if(count<=17600):\n",
    "#             train_val_data.append(obj)\n",
    "        all_data.append(obj)\n",
    "random.shuffle(all_data)\n",
    "# all_length = len(all_data)\n",
    "# print(all_length) \n",
    "# train_val_data = all_data[:int(0.9*complete_length)]\n",
    "# test_data = all_data[int(0.9*complete_length)+1:]\n",
    "count = 0\n",
    "truth_data = []\n",
    "with jsonlines.open('truth.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        truth_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals = []\n",
    "data_df = pd.DataFrame.from_dict(all_data)\n",
    "truth_data_df = pd.DataFrame.from_dict(truth_data)\n",
    "train = pd.merge(data_df, truth_data_df, on=\"id\")\n",
    "features = [\"id\", \"postMedia\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\", \"truthClass\"]\n",
    "vals_train = train[features].values.tolist()\n",
    "for i in range(len(vals_train)):\n",
    "    if vals_train[i][1] != []:\n",
    "        train_vals.append([vals_train[i][0], [vals_train[i][1][0]], vals_train[i][2], vals_train[i][3], vals_train[i][4], vals_train[i][5], vals_train[i][6], vals_train[i][7], vals_train[i][8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length 10306\n"
     ]
    }
   ],
   "source": [
    "vals_train_df = pd.DataFrame(train_vals, columns=[\"id\", \"postMedia\", \"postText\", \"targetCaptions\", \"targetParagraphs\", \"targetTitle\", \"targetKeywords\",\n",
    "                \"targetDescription\", \"truthClass\"])\n",
    "print(\"Training data length\", len(train_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the CNN from ./vgg16_no_fc.npy...\n",
      "conv3_3\n",
      "conv1_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 2/13 [00:00<00:01,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv3_1\n",
      "conv5_2\n",
      "conv1_1\n",
      "conv4_3\n",
      "conv4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv5_3\n",
      "conv2_1\n",
      "conv2_2\n",
      "conv3_2\n",
      "conv5_1\n",
      "conv4_2\n",
      "26 tensors loaded.\n"
     ]
    }
   ],
   "source": [
    "image_features_train = imgModel(vals_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10306\n"
     ]
    }
   ],
   "source": [
    "print(len(image_features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(vals_df):\n",
    "    labels = []\n",
    "    for i in vals_df.values:\n",
    "        if(i[8]==\"clickbait\"):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(vals_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10306\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16652 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "title_train = get_title_df(vals_train_df.values.tolist())\n",
    "title_train_df, t_train_word_index = get_padded_sequences(title_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 272530 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "content_train = get_content_df(vals_train_df.values.tolist())\n",
    "content_train_df, c_train_word_index = get_padded_sequences(content_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (10306, 3)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels), num_classes=3)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = len(title_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.8\n",
    "TRAIN_VAL_SPLIT = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(title_train_df.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = title_train_df[indices]\n",
    "contentdata = content_train_df[indices]\n",
    "labels = labels[indices]\n",
    "images = np.asarray(image_features_train)[indices]\n",
    "nb_validation_samples = int(TRAIN_SPLIT * title_train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:nb_validation_samples]\n",
    "c_train = contentdata[:nb_validation_samples]\n",
    "y_train = labels[:nb_validation_samples]\n",
    "x_val = data[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "c_val = contentdata[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "y_val = labels[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "x_test = data[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "c_test = contentdata[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "y_test = labels[int(TRAIN_VAL_SPLIT*train_length):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = images[:nb_validation_samples]\n",
    "image_val = images[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "image_test = images[int(TRAIN_VAL_SPLIT*train_length):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8244\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8244\n"
     ]
    }
   ],
   "source": [
    "print(len(image_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f=open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embedding_matrix = np.random.random((len(t_train_word_index) + 1, EMBEDDING_DIM)) ##Titles\n",
    "for word, i in t_train_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        t_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_embedding_matrix = np.random.random((len(c_train_word_index) + 1, EMBEDDING_DIM)) ##Content\n",
    "for word, i in c_train_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        c_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = np.concatenate((t_embedding_matrix, c_embedding_matrix), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(t_train_word_index)+ len(c_train_word_index) + 2, EMBEDDING_DIM, weights=[weights_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "content_data_input = Input(shape=(MAX_CONTENT_LENGTH,),dtype='float32')\n",
    "image_data_input = Input(shape=(100352,), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/layers/recurrent.py:2024: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n"
     ]
    }
   ],
   "source": [
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/layers/recurrent.py:2024: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n"
     ]
    }
   ],
   "source": [
    "content_embedded_sequences = embedding_layer(content_data_input)\n",
    "l_lstm_content = Bidirectional(LSTM(100))(content_embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_title = Dense(1, activation='softmax')(l_lstm) \n",
    "\n",
    "preds_content = Dense(1, activation='softmax')(l_lstm_content)\n",
    "\n",
    "preds_image = Dense(1, activation='softmax')(image_data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwini/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "model1 = Model(sequence_input, preds_title)\n",
    "model2 = Model(content_data_input, preds_content)\n",
    "model3 = Model(image_data_input, preds_image)\n",
    "# merged = Merge([model1, model3], mode='concat', name=\"merged\")\n",
    "merged = Merge([model1, model2, model3], mode='concat', name=\"merged\")\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merged (Merge)               (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 29,340,755\n",
      "Trainable params: 29,340,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"weights-title+img-{epoch:02d}-{val_acc:.2f}.hdf5\")\n",
    "callbacks_list = [checkpoint]\n",
    "final_model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model.fit([x_train, np.array(image_train)], y_train, validation_data=([x_val, np.array(image_val)], y_val), epochs=10, batch_size=32, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit([x_train, c_train, np.asarray(image_train)], y_train, validation_data=([x_val, c_val, np.asarray(image_val)], y_val), epochs=2, batch_size=32, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(c_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit([x_train, c_train], y_train, validation_data=([x_val, c_val], y_val), epochs=2, batch_size=32, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.load_weights('weights-title+contentimg-01-0.81.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findmaxindices(final_model, number, x_u, image_features_u):\n",
    "    test_proba = final_model.predict_classes([x_u[:36032], np.array(image_features_u)][:36032], batch_size=32, verbose=1)\n",
    "    max_indices = np.argpartition(test_proba,-number,axis=0)[-number:]\n",
    "    return test_proba, max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findmaxindicesC(final_model, number, x_u, content):\n",
    "    test_proba = final_model.predict_classes([x_u[:36032], content[:36032]], batch_size=32, verbose=1)\n",
    "    max_indices = np.argpartition(test_proba,-number,axis=0)[-number:]\n",
    "    return test_proba, max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findmaxindicesCI(final_model, number, x_u, content, image_features_u):\n",
    "    test_proba = final_model.predict_classes([x_u[:36032], content[:36032], np.array(image_features_u)[:36032]], batch_size=32, verbose=1)\n",
    "    max_indices = np.argpartition(test_proba,-number,axis=0)[-number:]\n",
    "    return test_proba, max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addthistotrain(indices, xtest, imgtest, ytest, xtrain, imgtrain, ytrain):\n",
    "        # getting all the y's\n",
    "#         ytest = self.get_y(xtest)\n",
    "        ytest = to_categorical(np.asarray(ytest), num_classes =2)\n",
    "        \n",
    "        # adding the min and max indices populations to xtrain and ytrain there\n",
    "        print(\"Ytest shape is \",ytest.shape)\n",
    "        print(\"Xtrain shape is\",xtrain.shape)\n",
    "        print(\"Xtest shape is \",xtest.shape)\n",
    "        print(\"Ytrain shape is \",ytrain.shape)\n",
    "        print(\"Indices shape is \",indices.shape)\n",
    "        print(\"xtest[indices] shape is\",xtest[indices].shape)\n",
    "        \n",
    "        xtrain = np.concatenate((xtrain,xtest[indices]),axis=0)\n",
    "        imgtrain = np.concatenate((imgtrain,imgtest[indices]),axis=0)\n",
    "        ytrain = np.concatenate((ytrain,ytest[indices]),axis=0)\n",
    "#         ytrain[ytrain>=0.5]=1\n",
    "#         ytrain[ytrain<0.5]=0\n",
    "        \n",
    "        print(\"################################\")\n",
    "        print(\"Ytest shape is \",ytest.shape)\n",
    "        print(\"Xtrain shape is\",xtrain.shape)\n",
    "        print(\"Xtest shape is \",xtest.shape)\n",
    "        print(\"Ytrain shape is \",ytrain.shape)\n",
    "        print(\"Indices shape is \",indices.shape)\n",
    "        print(\"xtest[indices] shape is\",xtest[indices].shape)\n",
    "        \n",
    "        return (xtrain, imgtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addthistotrainC(indices, xtest, contenttest, ytest, xtrain, contenttrain, ytrain):\n",
    "        # getting all the y's\n",
    "#         ytest = self.get_y(xtest)\n",
    "        ytest = to_categorical(np.asarray(ytest), num_classes =2)\n",
    "        \n",
    "        # adding the min and max indices populations to xtrain and ytrain there\n",
    "        print(\"Ytest shape is \",ytest.shape)\n",
    "        print(\"Xtrain shape is\",xtrain.shape)\n",
    "        print(\"Xtest shape is \",xtest.shape)\n",
    "        print(\"Ytrain shape is \",ytrain.shape)\n",
    "        print(\"Indices shape is \",indices.shape)\n",
    "        print(\"xtest[indices] shape is\",xtest[indices].shape)\n",
    "        \n",
    "        xtrain = np.concatenate((xtrain,xtest[indices]),axis=0)\n",
    "        contenttrain = np.concatenate((contenttrain,contenttest[indices]),axis=0)\n",
    "        ytrain = np.concatenate((ytrain,ytest[indices]),axis=0)\n",
    "#         ytrain[ytrain>=0.5]=1\n",
    "#         ytrain[ytrain<0.5]=0\n",
    "        \n",
    "        print(\"################################\")\n",
    "        print(\"Ytest shape is \",ytest.shape)\n",
    "        print(\"Xtrain shape is\",xtrain.shape)\n",
    "        print(\"Xtest shape is \",xtest.shape)\n",
    "        print(\"Ytrain shape is \",ytrain.shape)\n",
    "        print(\"Indices shape is \",indices.shape)\n",
    "        print(\"xtest[indices] shape is\",xtest[indices].shape)\n",
    "        \n",
    "        return (xtrain, contenttrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addthistotrainCI(indices, xtest, contenttest, imgtest, ytest, xtrain, contenttrain, imgtrain, ytrain):\n",
    "        # getting all the y's\n",
    "#         ytest = self.get_y(xtest)\n",
    "        ytest = to_categorical(np.asarray(ytest), num_classes =3)\n",
    "        \n",
    "        # adding the min and max indices populations to xtrain and ytrain there\n",
    "        print(\"Ytest shape is \",ytest.shape)\n",
    "        print(\"Xtrain shape is\",xtrain.shape)\n",
    "        print(\"Xtest shape is \",xtest.shape)\n",
    "        print(\"Ytrain shape is \",ytrain.shape)\n",
    "        print(\"Indices shape is \",indices.shape)\n",
    "        print(\"xtest[indices] shape is\",xtest[indices].shape)\n",
    "        \n",
    "        xtrain = np.concatenate((xtrain,xtest[indices]),axis=0)\n",
    "        contenttrain = np.concatenate((contenttrain,contenttest[indices]),axis=0)\n",
    "        imgtrain = np.concatenate((imgtrain,imgtest[indices]),axis=0)\n",
    "        ytrain = np.concatenate((ytrain,ytest[indices]),axis=0)\n",
    "#         ytrain[ytrain>=0.5]=1\n",
    "#         ytrain[ytrain<0.5]=0\n",
    "        \n",
    "        print(\"################################\")\n",
    "        print(\"Ytest shape is \",ytest.shape)\n",
    "        print(\"Xtrain shape is\",xtrain.shape)\n",
    "        print(\"Xtest shape is \",xtest.shape)\n",
    "        print(\"Ytrain shape is \",ytrain.shape)\n",
    "        print(\"Indices shape is \",indices.shape)\n",
    "        print(\"xtest[indices] shape is\",xtest[indices].shape)\n",
    "        \n",
    "        return (xtrain, contenttrain, imgtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36032/36032 [==============================] - 1229s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "ytest, max_indices = findmaxindicesCI(final_model, 20000, title_unlabeled_df, content_unlabeled_df, image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-bb300876137d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindmaxindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_unlabeled_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-6f85b98bb716>\u001b[0m in \u001b[0;36mfindmaxindices\u001b[0;34m(final_model, number, x_u, image_features_u)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfindmaxindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtest_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_u\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m36032\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m36032\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmax_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_proba\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ytest, max_indices = findmaxindices(final_model, 20000, title_unlabeled_df, image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest, max_indices = findmaxindicesC(final_model, 20000, title_unlabeled_df, content_unlabeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ytest[max_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = x_train\n",
    "imagetrain = image_train\n",
    "ytrain = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = data\n",
    "contenttrain = contentdata\n",
    "imagetrain = images\n",
    "ytrain = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title_unlabeled_df[max_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newXtrain, newImgtrain, newYtrain = addthistotrain(max_indices, title_unlabeled_df, np.array(image_features), ytest, xtrain, np.array(imagetrain), ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newXtrain, newContenttrain, newYtrain = addthistotrainC(max_indices, title_unlabeled_df, content_unlabeled_df, ytest, xtrain, contenttrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-e973fb8d32f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewContenttrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewImgtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewYtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddthistotrainCI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_unlabeled_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_unlabeled_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontenttrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagetrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "newXtrain, newContenttrain, newImgtrain, newYtrain = addthistotrainCI(max_indices, title_unlabeled_df, content_unlabeled_df, np.array(image_features), ytest, xtrain, contenttrain, np.array(imagetrain), ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(imagetrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(title_unlabeled_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shufflensplit(xtrain, imgtrain, ytrain):\n",
    "    indices = np.arange(xtrain.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = xtrain[indices]\n",
    "    labels = ytrain[indices]\n",
    "    images = np.asarray(imgtrain)[indices]\n",
    "    nb_validation_samples = int(TRAIN_SPLIT * len(xtrain))\n",
    "    train_length = len(xtrain)\n",
    "    x_train = data[0:nb_validation_samples]\n",
    "    y_train = labels[0:nb_validation_samples]\n",
    "    x_val = data[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    y_val = labels[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    x_test = data[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    y_test = labels[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    image_train = images[0:nb_validation_samples]\n",
    "    image_val = images[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    image_test = images[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    return(x_train, y_train, x_val, y_val, x_test, y_test, image_train, image_val, image_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shufflensplitC(xtrain, contenttrain, ytrain):\n",
    "    indices = np.arange(xtrain.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = xtrain[indices]\n",
    "    content = contenttrain[indices]\n",
    "    labels = ytrain[indices]\n",
    "#     images = np.asarray(imgtrain)[indices]\n",
    "    nb_validation_samples = int(TRAIN_SPLIT * len(xtrain))\n",
    "    train_length = len(xtrain)\n",
    "    x_train = data[0:nb_validation_samples]\n",
    "    c_train = content[0:nb_validation_samples]\n",
    "    y_train = labels[0:nb_validation_samples]\n",
    "    x_val = data[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    c_val = content[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    y_val = labels[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    x_test = data[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    c_test = content[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    y_test = labels[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "#     image_train = images[0:nb_validation_samples]\n",
    "#     image_val = images[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "#     image_test = images[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    return(x_train, y_train, x_val, y_val, x_test, y_test, c_train, c_val, c_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shufflensplitCI(xtrain, contenttrain, imgtrain, ytrain):\n",
    "    indices = np.arange(xtrain.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = xtrain[indices]\n",
    "    content = contenttrain[indices]\n",
    "    labels = ytrain[indices]\n",
    "    images = np.asarray(imgtrain)[indices]\n",
    "    nb_validation_samples = int(TRAIN_SPLIT * len(xtrain))\n",
    "    train_length = len(xtrain)\n",
    "    x_train = data[0:nb_validation_samples]\n",
    "    c_train = content[0:nb_validation_samples]\n",
    "    y_train = labels[0:nb_validation_samples]\n",
    "    x_val = data[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    c_val = content[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    y_val = labels[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    x_test = data[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    c_test = content[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    y_test = labels[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    image_train = images[0:nb_validation_samples]\n",
    "    image_val = images[nb_validation_samples:int(TRAIN_VAL_SPLIT*train_length)]\n",
    "    image_test = images[int(TRAIN_VAL_SPLIT*train_length):]\n",
    "    return(x_train, y_train, x_val, y_val, x_test, y_test, c_train, c_val, c_test, image_train, image_val, image_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Xtrain, new_Ytrain, new_Xval, new_Yval, x_test, y_test, new_Ctrain, new_Cval, c_test, new_Imgtrain, new_Imgval, image_test = shufflensplitCI(newXtrain, newContenttrain, newImgtrain, newYtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_Ctrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_Xval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_supervised_model = Sequential()\n",
    "semi_supervised_model.add(merged)\n",
    "semi_supervised_model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_semi = ModelCheckpoint(\"weights-semi+title+contentimg-{epoch:02d}-{val_acc:.2f}.hdf5\")\n",
    "callbacks_list_semi = [checkpoint_semi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "semi_supervised_model.fit([new_Xtrain, new_Imgtrain], new_Ytrain, validation_data=([new_Xval, np.array(new_Imgval)], new_Yval), epochs=3, batch_size=32, callbacks=callbacks_list_semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "semi_supervised_model.fit([new_Xtrain, new_Ctrain], new_Ytrain, validation_data=([new_Xval, new_Cval], new_Yval), epochs=2, batch_size=32, callbacks=callbacks_list_semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "semi_supervised_model.fit([new_Xtrain, new_Ctrain, new_Imgtrain], new_Ytrain, validation_data=([new_Xval, new_Cval, np.array(new_Imgval)], new_Yval), epochs=2, batch_size=32, callbacks=callbacks_list_semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "semi_supervised_model.load_weights('weights-semi+title+contentimg-01-0.89.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = semi_supervised_model.predict_classes([x_test, np.asarray(image_test)], batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = semi_supervised_model.predict_classes([x_test, c_test], batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2031/2031 [==============================] - 35s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = semi_supervised_model.predict_classes([x_test, c_test, np.asarray(image_test)], batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(y_test,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on Test data  0.899064500246\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score on Test data \", accuracy_score(np.argmax(y_test,1), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
